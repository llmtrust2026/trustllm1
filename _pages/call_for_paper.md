---
layout: page
title: "Call for Papers"
permalink: /call-for-papers/
nav: true
nav_order: 3
nav_title: "CFP"
---

# Call for Papers

**LLMTrust 2026: The 1st International Workshop on Software Engineering for and with Trustworthy LLMs** invites original contributions on engineering trustworthy LLM-based systems and the rigorous, auditable use of LLMs across the software lifecycle. The workshop emphasizes actionable techniques, evidence (tests, metrics, logs), open artifacts, and alignment with standards (e.g., NIST AI RMF, ISO/IEC, EU AI Act). 

---

## Topics of Interest

We welcome submissions addressing either or both complementary scopes, plus cross-cutting themes: 

**SE for Trustworthy LLM-Based Systems** (engineering applications that embed/deploy LLMs)
- Architecture & design patterns for RAG, agentic, and multi-modal systems  
- Testing, debugging, and validation of LLM applications  
- Robustness & adversarial evaluation: jailbreaks, prompt injection, red-teaming  
- Grounding, hallucination mitigation, and provenance guarantees  
- Runtime monitoring, observability, and incident response  
- Safety guardrails, tool-use controls, and constraints for agentic AI  
- Benchmarking & reliability metrics (SLAs/SLOs, evaluation suites)  
- Data governance, lineage tracking, and reproducibility in training/fine-tuning  
- Privacy-preserving design, secure model serving, and regulatory compliance  

**SE with Trustworthy LLM Integration** (responsible use of LLMs across the lifecycle)
- Provenance tracking for LLM-generated code, design decisions, and requirements  
- Human-over-the-loop oversight in coding, testing, and review  
- Safety & quality assurance for LLM-generated artifacts  
- Developer agency and skill preservation in LLM-augmented workflows  
- Empirical studies on adoption: productivity, quality, security  
- Bias, fairness, transparency in LLM-assisted decisions  
- LLMOps integration: policy gates, compliance, and risk management in CI/CD  
- Education, training, and best practices for responsible LLM use  

**Cross-Cutting**
- Governance frameworks & standards alignment (NIST AI RMF, ISO/IEC, EU AI Act)  
- Tool demos, datasets, benchmarks, reusable artifacts; case studies and industry experience; interdisciplinary perspectives across AI ethics, SE, HCI, and policy.  

---

## Submission & Proceedings

- **Venue & Proceedings.** Proceedings will appear in the **ACM Digital Library** as part of the **FSE 2026 Companion Proceedings**. All submissions must use the **ACM format** and follow ACM policies.  
- **Submission Types.**  
  - **Full papers:** **8 pages** + up to **2 pages** references  
  - **Short papers:** **5 pages** + up to **1 page** references  
  - **Extended abstracts:** **5 pages** + up to **1 page** references (**APC-free**) 
- **Registration.** At least one author of each accepted paper must register for the workshop. Organizers will not submit to their own workshop (FSE policy). 
- **Submission Site.** Please submit via **HotCRP**: https://fse26workshops.hotcrp.com/ 


---

## Important Dates (AoE)

- **Paper submission:** **February 12, 2026**  
- **Notification:** **March 19, 2026**  
- **Camera-ready:** **April 2, 2026**  
- **Workshop:** **July 5 or 6, 2026** 

---

## Paper Presentation & Policies

- Accepted papers will be presented in the workshop’s **full-day program** with keynotes, paper sessions, tool demos, and a panel with breakouts. 
- **Policy on Human Participants.** “As a published ACM author, you and your co-authors are subject to all ACM Publications Policies, including ACM’s Publications Policy on Research Involving Human Participants and Subjects.” 
- **ACM Open & APCs.** FSE 2026 uses ACM Open. Authors at participating institutions are covered; others may incur APCs with temporary subsidies; **extended abstracts remain APC-free**. Please consult ACM Open participation and waiver policies. 

---

### Contact

**Sumon Biswas** (Case Western Reserve University) — sumon@case.edu  
**Shibbir Ahmed** (Texas State University) — shibbir@txstate.edu :contentReference[oaicite:11]{index=11}
